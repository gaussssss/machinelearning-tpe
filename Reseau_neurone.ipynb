{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Reseau_neurone.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gaussssss/machinelearning-tpe/blob/main/Reseau_neurone.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>NoteBook de TPE d'apprentissage artificiel UY I Niveau 3 Info 2021/2022</h1>\n",
        "\n",
        "<h3>Liste des membres du TPE</h3>\n",
        "<ul>\n",
        "  <li>MOMENE TIYA Florian <strong>21S2785</strong></li>\n",
        "  <li>TEIDA NOUTSA Remi Raoul <strong>19M2447</strong></li>\n",
        "  <li>MBOCK NYENGUE Anne Claude <strong>19M2211</strong></li>\n",
        "  <li>EKOH FOUAPON NJIKAM Yvan <strong>19M2198</strong></li>\n",
        "</ul>\n",
        "<br>\n",
        "<strong><i>Par Dr. Melatagia</i></strong>"
      ],
      "metadata": {
        "id": "e-0HTbTcEOfs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Entrainement des réseaux de neurone en python"
      ],
      "metadata": {
        "id": "3rR93CoiC51w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pour la première portion du code nous allons créer un tableau à deux colonne ou la première colonne est le vecteur qui contient les entrées de notre reseaux de neurone et la deuxième conlonne sera un vecteur contenant les différentes étiquettes de notre jeu de donnée d'entrainement.Pour ce faire nous allons utiliser la fonction reshape de la librairie numpy.Enfin à la sortir de notre reseaux de neurone,nous allons évaluer l'érreur qui est la différence entre l'étiquette attendu et l'étiquette prédit."
      ],
      "metadata": {
        "id": "r2SNC3xMDw4O"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FP1YZ7gZC5GX"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy.special import expit as activation_function\n",
        "from scipy.stats import truncnorm\n",
        "\n",
        "def truncated_normal(mean=0, sd=1, low=0, upp=10):\n",
        "  return truncnorm(\n",
        "  (low - mean) / sd, (upp - mean) / sd, loc=mean, scale=sd)\n",
        "class NeuralNetwork:\n",
        "  def __init__(self,\n",
        "    no_of_in_nodes,\n",
        "    no_of_out_nodes,\n",
        "    no_of_hidden_nodes,\n",
        "    learning_rate):\n",
        "    self.no_of_in_nodes = no_of_in_nodes\n",
        "    self.no_of_out_nodes = no_of_out_nodes\n",
        "    self.no_of_hidden_nodes = no_of_hidden_nodes\n",
        "    self.learning_rate = learning_rate\n",
        "    self.create_weight_matrices()\n",
        "  def create_weight_matrices(self):\n",
        "  #\"\"\" A method to initialize the weight matrices of the neural network\"\"\"\n",
        "    rad = 1 / np.sqrt(self.no_of_in_nodes)\n",
        "    X = truncated_normal(mean=0, sd=1, low=-rad, upp=rad)\n",
        "    self.weights_in_hidden = X.rvs((self.no_of_hidden_nodes,\n",
        "    self.no_of_in_nodes))\n",
        "    rad = 1 / np.sqrt(self.no_of_hidden_nodes)\n",
        "    X = truncated_normal(mean=0, sd=1, low=-rad, upp=rad)\n",
        "    self.weights_hidden_out = X.rvs((self.no_of_out_nodes,\n",
        "    self.no_of_hidden_nodes))\n",
        "  def train(self, input_vector, target_vector):\n",
        "    #\"\"\"input_vector and target_vector can be tuples, lists or ndarrays\"\"\"\n",
        "    # make sure that the vectors have the right shape\n",
        "    input_vector = np.array(input_vector)\n",
        "    input_vector = input_vector.reshape(input_vector.size, 1)\n",
        "    target_vector = np.array(target_vector).reshape(target_vector.size, 1)\n",
        "    output_vector_hidden = activation_function(self.weights_in_hidden @ input_vector)\n",
        "    output_vector_network = activation_function(self.weights_hidden_out @ output_vector_hidden)\n",
        "    output_error = target_vector - output_vector_network\n",
        "    tmp = output_error * output_vector_network * (1.0 - output_vector_network)\n",
        "    self.weights_hidden_out += self.learning_rate * (tmp @ output_vector_hidden.T)\n",
        "    # calculate hidden errors:\n",
        "    hidden_errors = self.weights_hidden_out.T @ output_error\n",
        "    # update the weights:\n",
        "    tmp = hidden_errors * output_vector_hidden * (1.0 - output_vector_hidden)\n",
        "    self.weights_in_hidden += self.learning_rate * (tmp @ input_vector.T)\n",
        "  def run(self, input_vector):\n",
        "    #\"\"\"running the network with an input vector 'input_vector'.'input_vector' can be tuple, list or ndarray\"\"\"\n",
        "    # make sure that input_vector is a column vector:\n",
        "    input_vector = np.array(input_vector)\n",
        "    input_vector = input_vector.reshape(input_vector.size, 1)\n",
        "    input4hidden = activation_function(self.weights_in_hidden @ input_vector)\n",
        "    output_vector_network = activation_function(self.weights_hidden_out @ input4hidden)\n",
        "    return output_vector_network\n",
        "  def evaluate(self, data, labels):\n",
        "   # \"\"\"\n",
        "    #Counts how often the actual result corresponds to the\n",
        "    #target result.\n",
        "    #A result is considered to be correct, if the index of\n",
        "    #the maximal value corresponds to the index with the \"1\"\n",
        "    #in the one-hot representation,\n",
        "    #e.g.\n",
        "    #res = [0.1, 0.132, 0.875]\n",
        "    #labels[i] = [0, 0, 1]\n",
        "    #\"\"\"\n",
        "    corrects, wrongs = 0, 0\n",
        "    for i in range(len(data)):\n",
        "      res = self.run(data[i])\n",
        "      res_max = res.argmax()\n",
        "      if res_max == labels[i].argmax():\n",
        "        corrects += 1\n",
        "      else:\n",
        "        wrongs += 1\n",
        "    return corrects, wrongs\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pour l'exercution du contenu de notre classe NeuralNetwork,nous allons créer un jeu de donnée grace à la fonction  make_blobs du module sklearn.datasets qui prend comme paramètre le nombre d'échantillon total a généré, le nombre de catégorie ,ainsi qu'un paramètre qui va initialiser le générateur aléatoire.\n"
      ],
      "metadata": {
        "id": "On44asQaIPDH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_blobs\n",
        "n_samples = 500\n",
        "blob_centers = ([2, 6], [6, 2], [7, 7])\n",
        "n_classes = len(blob_centers)\n",
        "data, labels = make_blobs(n_samples=n_samples,\n",
        "centers=blob_centers,\n",
        "random_state=7)"
      ],
      "metadata": {
        "id": "OHDH_AajK-2X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualisation des données générées."
      ],
      "metadata": {
        "id": "nZvIlt0iLI0t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "colours = ('green', 'red', \"yellow\")\n",
        "fig, ax = plt.subplots()\n",
        "for n_class in range(n_classes):\n",
        "  ax.scatter(data[labels==n_class][:, 0],\n",
        "  data[labels==n_class][:, 1],\n",
        "  c=colours[n_class],\n",
        "  s=40,\n",
        "  label=str(n_class))\n"
      ],
      "metadata": {
        "id": "rnIWWQbDLUxl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pour échanger les étiquettes, nous pouvons utiliser la commande suivante:"
      ],
      "metadata": {
        "id": "sLIZTRJrL3tM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "labels = np.arange(n_classes) == labels.reshape(labels.size, 1)\n",
        "labels = labels.astype(np.float)\n",
        "labels[:7]\n"
      ],
      "metadata": {
        "id": "8-Ui3YqJMMqL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creation du jeu de donnée d'entrainement et de test grace à la fonction train_test_split du module sklearn.model_selection qui prend comme paramètre le jeu de donnée, les étiquettes, les proportions a reservé pour les données d'entrainements et de test ainsi qu'une valeur pour initialiser le générateur aléatoire."
      ],
      "metadata": {
        "id": "3bvVlyABMbT0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "res = train_test_split(data, labels,\n",
        "train_size=0.8,\n",
        "test_size=0.2,\n",
        "random_state=42)\n",
        "train_data, test_data, train_labels, test_labels = res\n",
        "train_labels[:10] #Affichage des 10 premiers étiquettes jeu de donnée d'entrainement."
      ],
      "metadata": {
        "id": "hG0xoId5MS8E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Le code si-dessous permet de créer un réseaux de neurone avec 2 entrées, 5 neurones sur la couche cachée et 3 neurones dans la couvhe de sorties en utilisant un taux d'apprentissage de 0.3.\n",
        "\n"
      ],
      "metadata": {
        "id": "gDbwTOwTO7Pa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "simple_network = NeuralNetwork(no_of_in_nodes=2,\n",
        "no_of_out_nodes=3,\n",
        "no_of_hidden_nodes=5,\n",
        "learning_rate=0.3)\n"
      ],
      "metadata": {
        "id": "J9_QWnJsOJ6C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Entrainement du réseaux de neurone avec le jeu de donnée d'entrainement et ces étiquettes."
      ],
      "metadata": {
        "id": "Xx3I_QIPPcUZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(train_data)):\n",
        "  simple_network.train(train_data[i], train_labels[i])\n"
      ],
      "metadata": {
        "id": "AqwmuTWSPttg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluation du réseau neuronal avec les données d'entrainement ainsi que leurs étiquettes."
      ],
      "metadata": {
        "id": "dbDco74mQEox"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "simple_network.evaluate(train_data, train_labels)"
      ],
      "metadata": {
        "id": "2z5g661oQVOt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implémentation d'un réseau de neurone biaisé"
      ],
      "metadata": {
        "id": "x0uL8-4YQrJ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.stats import truncnorm\n",
        "from scipy.special import expit as activation_function\n",
        "def truncated_normal(mean=0, sd=1, low=0, upp=10):\n",
        "  return truncnorm(\n",
        "    (low - mean) / sd, (upp - mean) / sd, loc=mean, scale=sd)\n"
      ],
      "metadata": {
        "id": "Bg8HpUgHSF8o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNetwork:\n",
        "  def __init__(self,\n",
        "    no_of_in_nodes,\n",
        "    no_of_out_nodes,\n",
        "    no_of_hidden_nodes,\n",
        "    learning_rate,\n",
        "    bias=None):\n",
        "    self.no_of_in_nodes = no_of_in_nodes\n",
        "    self.no_of_hidden_nodes = no_of_hidden_nodes\n",
        "    self.no_of_out_nodes = no_of_out_nodes\n",
        "    self.learning_rate = learning_rate\n",
        "    self.bias = bias\n",
        "    self.create_weight_matrices()\n",
        "  def create_weight_matrices(self):\n",
        "    #\"\"\" A method to initialize the weight matrices of the neur\n",
        "    #al\n",
        "    #network with optional bias nodes\"\"\"\n",
        "    bias_node = 1 if self.bias else 0\n",
        "    rad = 1 / np.sqrt(self.no_of_in_nodes + bias_node)\n",
        "    X = truncated_normal(mean=0, sd=1, low=-rad, upp=rad)\n",
        "    self.weights_in_hidden = X.rvs((self.no_of_hidden_nodes,\n",
        "    self.no_of_in_nodes + bias_node))\n",
        "    rad = 1 / np.sqrt(self.no_of_hidden_nodes + bias_node)\n",
        "    X = truncated_normal(mean=0, sd=1, low=-rad, upp=rad)\n",
        "    self.weights_hidden_out = X.rvs((self.no_of_out_nodes,\n",
        "    self.no_of_hidden_nodes\n",
        "    + bias_node))\n",
        "  def train(self, input_vector, target_vector):\n",
        "    #\"\"\" input_vector and target_vector can be tuple, list or n\n",
        "    #darray \"\"\"\n",
        "    # make sure that the vectors have the right shap\n",
        "    input_vector = np.array(input_vector)\n",
        "    input_vector = input_vector.reshape(input_vector.size,\n",
        "    1)\n",
        "    if self.bias:\n",
        "    # adding bias node to the end of the input_vector\n",
        "      input_vector = np.concatenate( (input_vector, [[self.bias]]) )\n",
        "    target_vector = np.array(target_vector).reshape(target_vector.size, 1)\n",
        "    output_vector_hidden = activation_function(self.weights_in_hidden @ input_vector)\n",
        "    if self.bias:\n",
        "      output_vector_hidden = np.concatenate( (output_vector_hidden, [[self.bias]]) )\n",
        "    output_vector_network = activation_function(self.weights_hidden_out @ output_vector_hidden)\n",
        "    output_error = target_vector - output_vector_network\n",
        "    # update the weights:\n",
        "    tmp = output_error * output_vector_network * (1.0 - output_vector_network)\n",
        "    self.weights_hidden_out += self.learning_rate * (tmp @ output_vector_hidden.T)\n",
        "    # calculate hidden errors:\n",
        "    hidden_errors = self.weights_hidden_out.T @ output_error\n",
        "    # update the weights:\n",
        "    tmp = hidden_errors * output_vector_hidden * (1.0 - output_vector_hidden)\n",
        "    if self.bias:\n",
        "      x = (tmp @input_vector.T)[:-1,:] # last row cut off,\n",
        "    else:\n",
        "      x = tmp @ input_vector.T\n",
        "    self.weights_in_hidden += self.learning_rate * x\n",
        "\n",
        "\n",
        "  def run(self, input_vector):\n",
        "      #\"\"\"\n",
        "      #running the network with an input vector 'input_vector'.\n",
        "      #'input_vector' can be tuple, list or ndarray\n",
        "    # \"\"\"\n",
        "      # make sure that input_vector is a column vector:\n",
        "    input_vector = np.array(input_vector)\n",
        "    input_vector = input_vector.reshape(input_vector.size, 1)\n",
        "    if self.bias:\n",
        "        # adding bias node to the end of the inpuy_vector\n",
        "      input_vector = np.concatenate( (input_vector, [[1]]) )\n",
        "    input4hidden = activation_function(self.weights_in_hidden @ input_vector)\n",
        "    if self.bias:\n",
        "      input4hidden = np.concatenate( (input4hidden, [[1]]) )\n",
        "    output_vector_network = activation_function(self.weights_hidden_out @ input4hidden)\n",
        "    return output_vector_network\n",
        "  def evaluate(self, data, labels):\n",
        "    corrects, wrongs = 0, 0\n",
        "    for i in range(len(data)):\n",
        "      res = self.run(data[i])\n",
        "      res_max = res.argmax()\n",
        "      if res_max == labels[i].argmax():\n",
        "        corrects += 1\n",
        "      else:\n",
        "        wrongs += 1\n",
        "    return corrects, wrongs\n"
      ],
      "metadata": {
        "id": "M706-uznSPWg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "simple_network = NeuralNetwork(no_of_in_nodes=2,no_of_out_nodes=3,no_of_hidden_nodes=5,learning_rate=0.1,bias=1)\n",
        "for i in range(len(train_data)):\n",
        "  simple_network.train(train_data[i], train_labels[i])\n",
        "simple_network.evaluate(train_data, train_labels)\n"
      ],
      "metadata": {
        "id": "HZuPYeIkUw3O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Utilisation de la fonction d'activation softmax"
      ],
      "metadata": {
        "id": "yH1ZT4-MXDlO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Création et exécution de la fonction softmax "
      ],
      "metadata": {
        "id": "jI2G_lpLXQc9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def softmax(x):\n",
        "  #\"\"\" applies softmax to an input x\"\"\"\n",
        "  e_x = np.exp(x)\n",
        "  return e_x / e_x.sum()\n",
        "x = np.array([1, 0, 3, 5])\n",
        "y = softmax(x)\n",
        "y, x / x.sum()\n"
      ],
      "metadata": {
        "id": "s1mdW2lMXLw6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "_tk8falvXh29"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def softmax(x):\n",
        "  #\"\"\" applies softmax to an input x\"\"\"\n",
        "  e_x = np.exp(x - np.max(x))\n",
        "  return e_x / e_x.sum()\n",
        "softmax(x)\n"
      ],
      "metadata": {
        "id": "wMAQqAkyX0kY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def softmax(x):\n",
        "  e_x = np.exp(x)\n",
        "  return e_x / e_x.sum()\n",
        "s = softmax(np.array([0, 4, 5]))\n",
        "si_sj = - s * s.reshape(3, 1)\n",
        "print(s)\n",
        "print(si_sj)\n",
        "s_der = np.diag(s) + si_sj\n",
        "s_der"
      ],
      "metadata": {
        "id": "1b6ytDAKX6Iz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.stats import truncnorm\n",
        "def truncated_normal(mean=0, sd=1, low=0, upp=10):\n",
        "  return truncnorm(\n",
        "  (low - mean) / sd, (upp - mean) / sd, loc=mean, scale=sd)\n",
        "@np.vectorize\n",
        "def sigmoid(x):\n",
        "  return 1 / (1 + np.e ** -x)\n",
        "def softmax(x):\n",
        "  e_x = np.exp(x)\n",
        "  return e_x / e_x.sum()\n",
        "class NeuralNetwork:\n",
        "  def __init__(self,\n",
        "    no_of_in_nodes,\n",
        "    no_of_out_nodes,\n",
        "    no_of_hidden_nodes,\n",
        "    learning_rate,\n",
        "    softmax=True):\n",
        "    self.no_of_in_nodes = no_of_in_nodes\n",
        "    self.no_of_out_nodes = no_of_out_nodes\n",
        "    self.no_of_hidden_nodes = no_of_hidden_nodes\n",
        "    self.learning_rate = learning_rate\n",
        "    self.softmax = softmax\n",
        "    self.create_weight_matrices()\n",
        "\n",
        "  def create_weight_matrices(self):\n",
        "    #\"\"\" A method to initialize the weight matrices of the neural network\"\"\"\n",
        "    rad = 1 / np.sqrt(self.no_of_in_nodes)\n",
        "    X = truncated_normal(mean=0, sd=1, low=-rad, upp=rad)\n",
        "    self.weights_in_hidden = X.rvs((self.no_of_hidden_nodes,\n",
        "    self.no_of_in_nodes))\n",
        "    rad = 1 / np.sqrt(self.no_of_hidden_nodes)\n",
        "    X = truncated_normal(mean=0, sd=1, low=-rad, upp=rad)\n",
        "    self.weights_hidden_out = X.rvs((self.no_of_out_nodes,\n",
        "    self.no_of_hidden_nodes))\n",
        "\n",
        "  def train(self, input_vector, target_vector):\n",
        "    #\"\"\"input_vector and target_vector can be tuples, lists or ndarrays\"\"\"\n",
        "    # make sure that the vectors have the right shape\n",
        "    input_vector = np.array(input_vector)\n",
        "    input_vector = input_vector.reshape(input_vector.size, 1)\n",
        "    target_vector = np.array(target_vector).reshape(target_vector.size, 1)\n",
        "    output_vector_hidden = sigmoid(self.weights_in_hidden @ input_vector)\n",
        "    if self.softmax:\n",
        "      output_vector_network = softmax(self.weights_hidden_out @ output_vector_hidden)\n",
        "    else:\n",
        "      output_vector_network = sigmoid(self.weights_hidden_out @ output_vector_hidden)\n",
        "    output_error = target_vector - output_vector_network\n",
        "    if self.softmax:\n",
        "      ovn = output_vector_network.reshape(output_vector_network.size,)\n",
        "      si_sj = - ovn * ovn.reshape(self.no_of_out_nodes, 1)\n",
        "      s_der = np.diag(ovn) + si_sj\n",
        "      tmp = s_der @ output_error\n",
        "      self.weights_hidden_out += self.learning_rate * (tmp\n",
        "      @ output_vector_hidden.T)\n",
        "    else:\n",
        "      tmp = output_error * output_vector_network * (1.0 - output_vector_network)\n",
        "      self.weights_hidden_out += self.learning_rate * (tmp @ output_vector_hidden.T)\n",
        "      # calculate hidden errors:\n",
        "    hidden_errors = self.weights_hidden_out.T @ output_error\n",
        "    # update the weights:\n",
        "    tmp = hidden_errors * output_vector_hidden * (1.0 - output_vector_hidden)\n",
        "    self.weights_in_hidden += self.learning_rate * (tmp @ input_vector.T)\n",
        "  def run(self, input_vector):\n",
        "    #\"\"\"\n",
        "    #running the network with an input vector 'input_vector'.\n",
        "    #'input_vector' can be tuple, list or ndarray\n",
        "    #\"\"\"\n",
        "    # make sure that input_vector is a column vector:\n",
        "    input_vector = np.array(input_vector)\n",
        "    input_vector = input_vector.reshape(input_vector.size, 1)\n",
        "    input4hidden = sigmoid(self.weights_in_hidden @ input_vector)\n",
        "    if self.softmax:\n",
        "      output_vector_network = softmax(self.weights_hidden_out @ input4hidden)\n",
        "    else:\n",
        "      output_vector_network = sigmoid(self.weights_hidden_out @ input4hidden)\n",
        "    return output_vector_network\n",
        "  def evaluate(self, data, labels):\n",
        "    corrects, wrongs = 0, 0\n",
        "    for i in range(len(data)):\n",
        "      res = self.run(data[i])\n",
        "      res_max = res.argmax()\n",
        "      if res_max == labels[i]:\n",
        "        corrects += 1\n",
        "      else:\n",
        "        wrongs += 1\n",
        "      return corrects, wrongs\n",
        "from sklearn.datasets import make_blobs\n",
        "n_samples = 300\n",
        "samples, labels = make_blobs(n_samples=n_samples,\n",
        "centers=([2, 6], [6, 2]),\n",
        "random_state=0)\n"
      ],
      "metadata": {
        "id": "JSE82MGVYFnW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "colours = ('green', 'red', 'blue', 'magenta', 'yellow', 'cyan')\n",
        "fig, ax = plt.subplots()\n",
        "for n_class in range(2):\n",
        "  ax.scatter(samples[labels==n_class][:, 0], samples[labels==n_class][:, 1],\n",
        "  c=colours[n_class], s=40, label=str(n_class))\n",
        "size_of_learn_sample = int(n_samples * 0.8)\n",
        "learn_data = samples[:size_of_learn_sample]\n",
        "test_data = samples[-size_of_learn_sample:]\n",
        "\n",
        "simple_network = NeuralNetwork(no_of_in_nodes=2,\n",
        "  no_of_out_nodes=2,\n",
        "  no_of_hidden_nodes=5,\n",
        "  learning_rate=0.3,\n",
        "  softmax=True)\n"
      ],
      "metadata": {
        "id": "vaVJrbUJaYUt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for x in [(1, 4), (2, 6), (3, 3), (6, 2)]:\n",
        "  y = simple_network.run(x)\n",
        "  print(x, y)"
      ],
      "metadata": {
        "id": "PBgkQC-_4EQT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels_one_hot = (np.arange(2) == labels.reshape(labels.size, 1))\n",
        "labels_one_hot = labels_one_hot.astype(np.float)\n",
        "for i in range(size_of_learn_sample):\n",
        "  #print(learn_data[i], labels[i], labels_one_hot[i])\n",
        "  simple_network.train(learn_data[i],\n",
        "  labels_one_hot[i])\n",
        "from collections import Counter\n",
        "evaluation = Counter()\n",
        "simple_network.evaluate(learn_data, labels)\n"
      ],
      "metadata": {
        "id": "Y2nc4d6BbJnl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Matrice de confusion"
      ],
      "metadata": {
        "id": "czSBEe0Wbl0q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "cm = np.array(\n",
        "[[5825, 1, 49, 23, 7, 46, 30, 12, 21, 26],\n",
        "[ 1, 6654, 48, 25, 10, 32, 19, 62, 111, 10],\n",
        "[ 2, 20, 5561, 69, 13, 10, 2, 45, 18, 2],\n",
        "[ 6, 26, 99, 5786, 5, 111, 1, 41, 110, 79],\n",
        "[ 4, 10, 43, 6, 5533, 32, 11, 53, 34, 79],\n",
        "[ 3, 1, 2, 56, 0, 4954, 23, 0, 12, 5],\n",
        "[ 31, 4, 42, 22, 45, 103, 5806, 3, 34, 3],\n",
        "[ 0, 4, 30, 29, 5, 6, 0, 5817, 2, 28],\n",
        "[ 35, 6, 63, 58, 8, 59, 26, 13, 5394, 24],\n",
        "[ 16, 16, 21, 57, 216, 68, 0, 219, 115, 5693]])\n"
      ],
      "metadata": {
        "id": "38qBNEadbqQ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def precision(label, confusion_matrix):\n",
        "  col = confusion_matrix[:, label]\n",
        "  return confusion_matrix[label, label] / col.sum()\n"
      ],
      "metadata": {
        "id": "Lp3SKlITb2gj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def recall(label, confusion_matrix):\n",
        "  row = confusion_matrix[label, :]\n",
        "  return confusion_matrix[label, label] / row.sum()\n"
      ],
      "metadata": {
        "id": "8NwLcyHob49Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def precision_macro_average(confusion_matrix):\n",
        "  rows, columns = confusion_matrix.shape\n",
        "  sum_of_precisions = 0\n",
        "  for label in range(rows):\n",
        "    sum_of_precisions += precision(label, confusion_matrix)\n",
        "  return sum_of_precisions / rows\n"
      ],
      "metadata": {
        "id": "mt16AIVxcBAR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def recall_macro_average(confusion_matrix):\n",
        "  rows, columns = confusion_matrix.shape\n",
        "  sum_of_recalls = 0\n",
        "  for label in range(columns):\n",
        "    sum_of_recalls += recall(label, confusion_matrix)\n",
        "  return sum_of_recalls / columns"
      ],
      "metadata": {
        "id": "hjcIkbs3cIS7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"label precision recall\")\n",
        "for label in range(10):\n",
        "  print(f\"{label:5d} {precision(label, cm):9.3f} {recall(label,cm):6.3f}\")\n"
      ],
      "metadata": {
        "id": "cUsFumq5cOVx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"precision total:\", precision_macro_average(cm))\n",
        "print(\"recall total:\", recall_macro_average(cm))\n"
      ],
      "metadata": {
        "id": "mKDxBdWncedB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Détermination de l'exactitude du modèle"
      ],
      "metadata": {
        "id": "V2vLSjPmcjCJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(confusion_matrix):\n",
        "  diagonal_sum = confusion_matrix.trace()\n",
        "  sum_of_all_elements = confusion_matrix.sum()\n",
        "  return diagonal_sum / sum_of_all_elements\n",
        "accuracy(cm)\n",
        "\n"
      ],
      "metadata": {
        "id": "7pxNmS9Kcqzq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Zmm6ojBkwshi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "image_size = 28 # width and length\n",
        "no_of_different_labels = 10 # i.e. 0, 1, 2, 3, ..., 9\n",
        "image_pixels = image_size * image_size\n",
        "data_path = \"/content/drive/MyDrive/Colab Notebooks/data1/\"\n",
        "train_data = np.loadtxt(data_path + \"mnist_train.csv\",delimiter=\",\")\n",
        "test_data = np.loadtxt(data_path + \"mnist_test.csv\",delimiter=\",\")\n",
        "test_data[:10]\n",
        "test_data[test_data==255]\n",
        "test_data.shape\n"
      ],
      "metadata": {
        "id": "YCZW94mDcxLx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fac = 0.99 / 255\n",
        "train_imgs = np.asfarray(train_data[:, 1:]) * fac + 0.01\n",
        "test_imgs = np.asfarray(test_data[:, 1:]) * fac + 0.01\n",
        "train_labels = np.asfarray(train_data[:, :1])\n",
        "test_labels = np.asfarray(test_data[:, :1])\n"
      ],
      "metadata": {
        "id": "yBmlp4X4d_qC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "lr = np.arange(10)\n",
        "for label in range(10):\n",
        "  one_hot = (lr==label).astype(np.int)\n",
        "  print(\"label: \", label, \" in one-hot representation: \", one_hot)\n"
      ],
      "metadata": {
        "id": "UrqB5hlWeBCo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr = np.arange(no_of_different_labels)\n",
        "# transform labels into one hot representation\n",
        "train_labels_one_hot = (lr==train_labels).astype(np.float)\n",
        "test_labels_one_hot = (lr==test_labels).astype(np.float)\n",
        "# we don't want zeroes and ones in the labels neither:\n",
        "train_labels_one_hot[train_labels_one_hot==0] = 0.01\n",
        "train_labels_one_hot[train_labels_one_hot==1] = 0.99\n",
        "test_labels_one_hot[test_labels_one_hot==0] = 0.01\n",
        "test_labels_one_hot[test_labels_one_hot==1] = 0.99\n"
      ],
      "metadata": {
        "id": "jFFItXcheKH4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(10):\n",
        "  img = train_imgs[i].reshape((28,28))\n",
        "  plt.imshow(img, cmap=\"Greys\")\n",
        "  plt.show()\n"
      ],
      "metadata": {
        "id": "etHKKehmeOdB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "with open(\"data/mnist/pickled_mnist.pkl\", \"bw\") as fh:\n",
        "  data = (train_imgs,\n",
        "    test_imgs,\n",
        "    train_labels,\n",
        "    test_labels,\n",
        "    train_labels_one_hot,\n",
        "    test_labels_one_hot)\n",
        "  pickle.dump(data, fh)"
      ],
      "metadata": {
        "id": "V6fayv1TeTmT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "with open(\"data/mnist/pickled_mnist.pkl\", \"br\") as fh:\n",
        "  data = pickle.load(fh)\n",
        "train_imgs = data[0]\n",
        "test_imgs = data[1]\n",
        "train_labels = data[2]\n",
        "test_labels = data[3]\n",
        "train_labels_one_hot = data[4]\n",
        "test_labels_one_hot = data[5]\n",
        "image_size = 28 # width and length\n",
        "no_of_different_labels = 10 # i.e. 0, 1, 2, 3, ..., 9\n",
        "image_pixels = image_size * image_size\n"
      ],
      "metadata": {
        "id": "Y0t6Ar7MeeRK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "@np.vectorize\n",
        "def sigmoid(x):\n",
        "  return 1 / (1 + np.e ** -x)\n",
        "activation_function = sigmoid\n",
        "from scipy.stats import truncnorm\n",
        "def truncated_normal(mean=0, sd=1, low=0, upp=10):\n",
        "  return truncnorm((low - mean) / sd,\n",
        "(upp - mean) / sd,\n",
        "loc=mean,\n",
        "scale=sd)\n",
        "class NeuralNetwork:\n",
        "  def __init__(self,\n",
        "    no_of_in_nodes,\n",
        "    no_of_out_nodes,\n",
        "    no_of_hidden_nodes,\n",
        "    learning_rate):\n",
        "    self.no_of_in_nodes = no_of_in_nodes\n",
        "    self.no_of_out_nodes = no_of_out_nodes\n",
        "    self.no_of_hidden_nodes = no_of_hidden_nodes\n",
        "    self.learning_rate = learning_rate\n",
        "    self.create_weight_matrices()\n",
        "  def create_weight_matrices(self):\n",
        "    #\"\"\"\n",
        "    #A method to initialize the weight\n",
        "    #matrices of the neural network\n",
        "    #\"\"\"\n",
        "    rad = 1 / np.sqrt(self.no_of_in_nodes)\n",
        "    X = truncated_normal(mean=0,\n",
        "    sd=1,\n",
        "    low=-rad,\n",
        "    upp=rad)\n",
        "    self.wih = X.rvs((self.no_of_hidden_nodes,\n",
        "    self.no_of_in_nodes))\n",
        "    rad = 1 / np.sqrt(self.no_of_hidden_nodes)\n",
        "    X = truncated_normal(mean=0, sd=1, low=-rad, upp=rad)\n",
        "    self.who = X.rvs((self.no_of_out_nodes,\n",
        "    self.no_of_hidden_nodes))\n",
        "  def train(self, input_vector, target_vector):\n",
        "    #\"\"\"\n",
        "    #input_vector and target_vector can\n",
        "    #be tuple, list or ndarray\n",
        "    #\"\"\"\n",
        "    input_vector = np.array(input_vector, ndmin=2).T\n",
        "    target_vector = np.array(target_vector, ndmin=2).T\n",
        "    output_vector1 = np.dot(self.wih,\n",
        "    input_vector)\n",
        "    output_hidden = activation_function(output_vector1)\n",
        "    output_vector2 = np.dot(self.who,\n",
        "    output_hidden)\n",
        "    output_network = activation_function(output_vector2)\n",
        "    output_errors = target_vector - output_network\n",
        "    # update the weights:\n",
        "    tmp = output_errors * output_network \\\n",
        "    * (1.0 - output_network)\n",
        "    tmp = self.learning_rate * np.dot(tmp,\n",
        "    output_hidden.T)\n",
        "    self.who += tmp\n",
        "    # calculate hidden errors:\n",
        "    hidden_errors = np.dot(self.who.T,\n",
        "    output_errors)\n",
        "    # update the weights:\n",
        "    tmp = hidden_errors * output_hidden * \\\n",
        "    (1.0 - output_hidden)\n",
        "    self.wih += self.learning_rate \\\n",
        "    * np.dot(tmp, input_vector.T)\n",
        "  def run(self, input_vector):\n",
        "    # input_vector can be tuple, list or ndarray\n",
        "    input_vector = np.array(input_vector, ndmin=2).T\n",
        "    output_vector = np.dot(self.wih,\n",
        "    input_vector)\n",
        "    output_vector = activation_function(output_vector)\n",
        "    output_vector = np.dot(self.who,\n",
        "    output_vector)\n",
        "    output_vector = activation_function(output_vector)\n",
        "    return output_vector\n",
        "  def confusion_matrix(self, data_array, labels):\n",
        "    cm = np.zeros((10, 10), int)\n",
        "    for i in range(len(data_array)):\n",
        "      res = self.run(data_array[i])\n",
        "      res_max = res.argmax()\n",
        "      target = labels[i][0]\n",
        "      cm[res_max, int(target)] += 1\n",
        "    return cm\n",
        "  def precision(self, label, confusion_matrix):\n",
        "    col = confusion_matrix[:, label]\n",
        "    return confusion_matrix[label, label] / col.sum()\n",
        "  def recall(self, label, confusion_matrix):\n",
        "    row = confusion_matrix[label, :]\n",
        "    return confusion_matrix[label, label] / row.sum()\n",
        "  def evaluate(self, data, labels):\n",
        "    corrects, wrongs = 0, 0\n",
        "    for i in range(len(data)):\n",
        "      res = self.run(data[i])\n",
        "      res_max = res.argmax()\n",
        "      if res_max == labels[i]:\n",
        "        corrects += 1\n",
        "      else:\n",
        "        wrongs += 1\n",
        "    return corrects, wrongs"
      ],
      "metadata": {
        "id": "hFP63bwDegZ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ANN = NeuralNetwork(no_of_in_nodes = image_pixels,\n",
        "no_of_out_nodes = 10,\n",
        "no_of_hidden_nodes = 100,\n",
        "learning_rate = 0.1)\n",
        "for i in range(len(train_imgs)):\n",
        "  ANN.train(train_imgs[i], train_labels_one_hot[i])\n"
      ],
      "metadata": {
        "id": "SucXUUxDgBl_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(20):\n",
        "  res = ANN.run(test_imgs[i])\n",
        "  print(test_labels[i], np.argmax(res), np.max(res))\n"
      ],
      "metadata": {
        "id": "40581T00gdJI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corrects, wrongs = ANN.evaluate(train_imgs, train_labels)\n",
        "print(\"accuracy train: \", corrects / ( corrects + wrongs))\n",
        "corrects, wrongs = ANN.evaluate(test_imgs, test_labels)\n",
        "print(\"accuracy: test\", corrects / ( corrects + wrongs))\n",
        "cm = ANN.confusion_matrix(train_imgs, train_labels)\n",
        "print(cm)\n",
        "for i in range(10):\n",
        "  print(\"digit: \", i, \"precision: \", ANN.precision(i, cm), \"recall: \", ANN.recall(i, cm))\n"
      ],
      "metadata": {
        "id": "UeJ0h66Hgpa3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 3\n",
        "NN = NeuralNetwork(no_of_in_nodes = image_pixels,\n",
        "no_of_out_nodes = 10,\n",
        "no_of_hidden_nodes = 100,\n",
        "learning_rate = 0.1)\n",
        "for epoch in range(epochs):\n",
        "  print(\"epoch: \", epoch)\n",
        "  for i in range(len(train_imgs)):\n",
        "    NN.train(train_imgs[i],\n",
        "    train_labels_one_hot[i])\n",
        "  corrects, wrongs = NN.evaluate(train_imgs, train_labels)\n",
        "  print(\"accuracy train: \", corrects / ( corrects + wrongs))\n",
        "  corrects, wrongs = NN.evaluate(test_imgs, test_labels)\n",
        "  print(\"accuracy: test\", corrects / ( corrects + wrongs))\n"
      ],
      "metadata": {
        "id": "BHrnDHAuguZn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "@np.vectorize\n",
        "def sigmoid(x):\n",
        "\n",
        "  return 1 / (1 + np.e ** -x)\n",
        "activation_function = sigmoid\n",
        "from scipy.stats import truncnorm\n",
        "def truncated_normal(mean=0, sd=1, low=0, upp=10):\n",
        "  return truncnorm((low - mean) / sd,\n",
        "  (upp - mean) / sd,\n",
        "  loc=mean,\n",
        "  scale=sd)\n",
        "class NeuralNetwork:\n",
        "  def __init__(self,\n",
        "      no_of_in_nodes,\n",
        "      no_of_out_nodes,\n",
        "      no_of_hidden_nodes,\n",
        "      learning_rate):\n",
        "    self.no_of_in_nodes = no_of_in_nodes\n",
        "    self.no_of_out_nodes = no_of_out_nodes\n",
        "    self.no_of_hidden_nodes = no_of_hidden_nodes\n",
        "    self.learning_rate = learning_rate\n",
        "    self.create_weight_matrices()\n",
        "  def create_weight_matrices(self):\n",
        "    #\"\"\" A method to initialize the weight matrices of the neur\n",
        "    #al network\"\"\"\n",
        "    rad = 1 / np.sqrt(self.no_of_in_nodes)\n",
        "    X = truncated_normal(mean=0,\n",
        "    sd=1,\n",
        "    low=-rad,\n",
        "    upp=rad)\n",
        "    self.wih = X.rvs((self.no_of_hidden_nodes,\n",
        "    self.no_of_in_nodes))\n",
        "    rad = 1 / np.sqrt(self.no_of_hidden_nodes)\n",
        "    X = truncated_normal(mean=0,\n",
        "    sd=1,\n",
        "    low=-rad,\n",
        "    upp=rad)\n",
        "    self.who = X.rvs((self.no_of_out_nodes,\n",
        "    self.no_of_hidden_nodes))\n",
        "  def train_single(self, input_vector, target_vector):\n",
        "    \n",
        "    #\"\"\"\n",
        "    #input_vector and target_vector can be tuple,\n",
        "    #list or ndarray\n",
        "    #\"\"\"\n",
        "    output_vectors = []\n",
        "    input_vector = np.array(input_vector, ndmin=2).T\n",
        "    target_vector = np.array(target_vector, ndmin=2).T\n",
        "    output_vector1 = np.dot(self.wih,\n",
        "    input_vector)\n",
        "    output_hidden = activation_function(output_vector1)\n",
        "    output_vector2 = np.dot(self.who,\n",
        "    output_hidden)\n",
        "    output_network = activation_function(output_vector2)\n",
        "    output_errors = target_vector - output_network\n",
        "    # update the weights:\n",
        "    tmp = output_errors * output_network * \\\n",
        "    (1.0 - output_network)\n",
        "    tmp = self.learning_rate * np.dot(tmp,\n",
        "    output_hidden.T)\n",
        "    self.who += tmp\n",
        "    # calculate hidden errors:\n",
        "    hidden_errors = np.dot(self.who.T,\n",
        "    output_errors)\n",
        "    # update the weights:\n",
        "    tmp = hidden_errors * output_hidden * (1.0 - output_hidden)\n",
        "    self.wih += self.learning_rate * np.dot(tmp, input_vector.T)\n",
        "  def train(self, data_array,\n",
        "    labels_one_hot_array,\n",
        "    epochs=1,\n",
        "    intermediate_results=False):\n",
        "    intermediate_weights = []\n",
        "    for epoch in range(epochs):\n",
        "      print(\"*\", end=\"\")\n",
        "      for i in range(len(data_array)):\n",
        "        self.train_single(data_array[i],\n",
        "        labels_one_hot_array[i])\n",
        "        if intermediate_results:\n",
        "          intermediate_weights.append((self.wih.copy(),\n",
        "          self.who.copy()))\n",
        "    return intermediate_weights\n",
        "  def confusion_matrix(self, data_array, labels):\n",
        "    cm = {}\n",
        "    for i in range(len(data_array)):\n",
        "      res = self.run(data_array[i])\n",
        "      res_max = res.argmax()\n",
        "      target = labels[i][0]\n",
        "      if (target, res_max) in cm:\n",
        "        cm[(target, res_max)] += 1\n",
        "      else:\n",
        "        cm[(target, res_max)] = 1\n",
        "    return cm\n",
        "  def run(self, input_vector):\n",
        "    #\"\"\" input_vector can be tuple, list or ndarray \"\"\"\n",
        "    input_vector = np.array(input_vector, ndmin=2).T\n",
        "    output_vector = np.dot(self.wih,\n",
        "    input_vector)\n",
        "    output_vector = activation_function(output_vector)\n",
        "    output_vector = np.dot(self.who,\n",
        "    output_vector)\n",
        "    output_vector = activation_function(output_vector)\n",
        "    return output_vector\n",
        "  def evaluate(self, data, labels):\n",
        "    corrects, wrongs = 0, 0\n",
        "    for i in range(len(data)):\n",
        "      res = self.run(data[i])\n",
        "      res_max = res.argmax()\n",
        "      if res_max == labels[i]:\n",
        "        corrects += 1\n",
        "      else:\n",
        "        wrongs += 1\n",
        "    return corrects, wrongs\n"
      ],
      "metadata": {
        "id": "-O4Cy71AhGU7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 10\n",
        "\n",
        "ANN = NeuralNetwork(no_of_in_nodes = image_pixels,no_of_out_nodes = 10,\n",
        "no_of_hidden_nodes = 100,\n",
        "learning_rate = 0.15)\n",
        "weights = ANN.train(train_imgs,\n",
        "train_labels_one_hot,\n",
        "epochs=epochs,\n",
        "intermediate_results=True)\n",
        "cm = ANN.confusion_matrix(train_imgs, train_labels)\n",
        "print(ANN.run(train_imgs[i]))\n"
      ],
      "metadata": {
        "id": "eJG3qJJD7Gfx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cm = list(cm.items())\n",
        "print(sorted(cm))\n"
      ],
      "metadata": {
        "id": "0AlBYZNDiygl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(epochs):\n",
        "  print(\"epoch: \", i)\n",
        "  ANN.wih = weights[i][0]\n",
        "  ANN.who = weights[i][1]\n",
        "  corrects, wrongs = ANN.evaluate(train_imgs, train_labels)\n",
        "  print(\"accuracy train: \", corrects / ( corrects + wrongs))\n",
        "  corrects, wrongs = ANN.evaluate(test_imgs, test_labels)\n",
        "  print(\"accuracy: test\", corrects / ( corrects + wrongs))"
      ],
      "metadata": {
        "id": "Ann6sZxzi_nN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Version biaisé"
      ],
      "metadata": {
        "id": "RfR42vDZjJj2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "@np.vectorize\n",
        "def sigmoid(x):\n",
        "  return 1 / (1 + np.e ** -x)\n",
        "  activation_function = sigmoid\n",
        "from scipy.stats import truncnorm\n",
        "def truncated_normal(mean=0, sd=1, low=0, upp=10):\n",
        "  return truncnorm((low - mean) / sd,\n",
        "  (upp - mean) / sd,\n",
        "  loc=mean,\n",
        "  scale=sd)\n",
        "class NeuralNetwork:\n",
        "  def __init__(self,\n",
        "    no_of_in_nodes,\n",
        "    no_of_out_nodes,\n",
        "    no_of_hidden_nodes,\n",
        "    learning_rate,\n",
        "    bias=None\n",
        "    ):\n",
        "    self.no_of_in_nodes = no_of_in_nodes\n",
        "    self.no_of_out_nodes = no_of_out_nodes\n",
        "    self.no_of_hidden_nodes = no_of_hidden_nodes\n",
        "    self.learning_rate = learning_rate\n",
        "    self.bias = bias\n",
        "    self.create_weight_matrices()\n",
        "  def create_weight_matrices(self):\n",
        "    #\"\"\"\n",
        "    #A method to initialize the weight\n",
        "    #matrices of the neural network with\n",
        "    #WITH BIAS NODES 209optional bias nodes\"\"\"\n",
        "    bias_node = 1 if self.bias else 0\n",
        "    rad = 1 / np.sqrt(self.no_of_in_nodes + bias_node)\n",
        "    X = truncated_normal(mean=0,\n",
        "    sd=1,\n",
        "    low=-rad,\n",
        "    upp=rad)\n",
        "    self.wih = X.rvs((self.no_of_hidden_nodes,\n",
        "    self.no_of_in_nodes + bias_node))\n",
        "    rad = 1 / np.sqrt(self.no_of_hidden_nodes + bias_node)\n",
        "    X = truncated_normal(mean=0, sd=1, low=-rad, upp=rad)\n",
        "    self.who = X.rvs((self.no_of_out_nodes,\n",
        "    self.no_of_hidden_nodes + bias_node))\n",
        "  def train(self, input_vector, target_vector):\n",
        "    #\"\"\"\n",
        "    #input_vector and target_vector can\n",
        "    #be tuple, list or ndarray\n",
        "    #\"\"\"\n",
        "    bias_node = 1 if self.bias else 0\n",
        "    if self.bias:\n",
        "    # adding bias node to the end of the inpuy_vector\n",
        "      input_vector = np.concatenate((input_vector,\n",
        "      [self.bias]) )\n",
        "    input_vector = np.array(input_vector, ndmin=2).T\n",
        "    target_vector = np.array(target_vector, ndmin=2).T\n",
        "    output_vector1 = np.dot(self.wih,\n",
        "    input_vector)\n",
        "    output_hidden = activation_function(output_vector1)\n",
        "    if self.bias:\n",
        "      output_hidden = np.concatenate((output_hidden,\n",
        "      [[self.bias]]) )\n",
        "    output_vector2 = np.dot(self.who,\n",
        "    output_hidden)\n",
        "    output_network = activation_function(output_vector2)\n",
        "    output_errors = target_vector - output_network\n",
        "    # update the weights:\n",
        "    tmp = output_errors * output_network * (1.0 - output_netwo\n",
        "    rk)\n",
        "    tmp = self.learning_rate * np.dot(tmp, output_hidden.T)\n",
        "    self.who += tmp\n",
        "    # calculate hidden errors:\n",
        "    hidden_errors = np.dot(self.who.T,\n",
        "    output_errors)\n",
        "    # update the weights:\n",
        "    tmp = hidden_errors * output_hidden * (1.0 - output_hidde\n",
        "    n)\n",
        "    if self.bias:\n",
        "      x = np.dot(tmp, input_vector.T)[:-1,:]\n",
        "    else:\n",
        "      x = np.dot(tmp, input_vector.T)\n",
        "    self.wih += self.learning_rate * x\n",
        "  def run(self, input_vector):\n",
        "    #\"\"\"\n",
        "    #input_vector can be tuple, list or ndarray\n",
        "    #\"\"\"\n",
        "    if self.bias:\n",
        "      # adding bias node to the end of the inpuy_vector\n",
        "      input_vector = np.concatenate((input_vector, [1]) )\n",
        "    input_vector = np.array(input_vector, ndmin=2).T\n",
        "    output_vector = np.dot(self.wih,\n",
        "    input_vector)\n",
        "    output_vector = activation_function(output_vector)\n",
        "    if self.bias:\n",
        "      output_vector = np.concatenate( (output_vector,\n",
        "      [[1]]) )\n",
        "    output_vector = np.dot(self.who,\n",
        "    output_vector)\n",
        "    output_vector = activation_function(output_vector)\n",
        "    return output_vector\n",
        "  def evaluate(self, data, labels):\n",
        "    corrects, wrongs = 0, 0\n",
        "    for i in range(len(data)):\n",
        "      res = self.run(data[i])\n",
        "      res_max = res.argmax()\n",
        "      if res_max == labels[i]:\n",
        "        corrects += 1\n",
        "      else:\n",
        "        wrongs += 1\n",
        "    return corrects, wrongs\n",
        "ANN = NeuralNetwork(no_of_in_nodes=image_pixels,\n",
        "no_of_out_nodes=10,\n",
        "no_of_hidden_nodes=200,\n",
        "learning_rate=0.1,\n",
        "bias=None)\n",
        "for i in range(len(train_imgs)):\n",
        "  ANN.train(train_imgs[i], train_labels_one_hot[i])\n",
        "for i in range(20):\n",
        "  res = ANN.run(test_imgs[i])\n",
        "  print(test_labels[i], np.argmax(res), np.max(res))\n"
      ],
      "metadata": {
        "id": "BPkpESyXjIdh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corrects, wrongs = ANN.evaluate(train_imgs, train_labels)\n",
        "print(\"accuracy train: \", corrects / ( corrects + wrongs))\n",
        "corrects, wrongs = ANN.evaluate(test_imgs, test_labels)\n",
        "print(\"accuracy: test\", corrects / ( corrects + wrongs))\n"
      ],
      "metadata": {
        "id": "xYvtvaY1kyYn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Version biaisé + epoch"
      ],
      "metadata": {
        "id": "fZzzInYEk842"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "@np.vectorize\n",
        "def sigmoid(x):\n",
        "return 1 / (1 + np.e ** -x)\n",
        "activation_function = sigmoid\n",
        "from scipy.stats import truncnorm\n",
        "def truncated_normal(mean=0, sd=1, low=0, upp=10):\n",
        "  return truncnorm((low - mean) / sd,\n",
        "  (upp - mean) / sd,\n",
        "  loc=mean,\n",
        "  scale=sd)\n",
        "class NeuralNetwork:\n",
        "  def __init__(self,\n",
        "    no_of_in_nodes,\n",
        "    no_of_out_nodes,\n",
        "    no_of_hidden_nodes,\n",
        "    learning_rate,\n",
        "    bias=None\n",
        "    ):\n",
        "    self.no_of_in_nodes = no_of_in_nodes\n",
        "    self.no_of_out_nodes = no_of_out_nodes\n",
        "    self.no_of_hidden_nodes = no_of_hidden_nodes\n",
        "    self.learning_rate = learning_rate\n",
        "    self.bias = bias\n",
        "    self.create_weight_matrices()\n",
        "  def create_weight_matrices(self):\n",
        "    #\"\"\"\n",
        "    #A method to initialize the weight matrices\n",
        "    #of the neural network with optional\n",
        "    #bias nodes\"\"\"\n",
        "    bias_node = 1 if self.bias else 0\n",
        "    rad = 1 / np.sqrt(self.no_of_in_nodes + bias_node)\n",
        "    X = truncated_normal(mean=0, sd=1, low=-rad, upp=rad)\n",
        "    self.wih = X.rvs((self.no_of_hidden_nodes,\n",
        "    self.no_of_in_nodes + bias_node))\n",
        "    rad = 1 / np.sqrt(self.no_of_hidden_nodes + bias_node)\n",
        "    X = truncated_normal(mean=0,\n",
        "    sd=1,\n",
        "    low=-rad,\n",
        "    upp=rad)\n",
        "    self.who = X.rvs((self.no_of_out_nodes,\n",
        "    WITH BIAS NODES 214\n",
        "    self.no_of_hidden_nodes + bias_node))\n",
        "  def train_single(self, input_vector, target_vector):\n",
        "    #\"\"\"\n",
        "    #input_vector and target_vector can be tuple,\n",
        "    #list or ndarray\n",
        "    #\"\"\"\n",
        "    bias_node = 1 if self.bias else 0\n",
        "    if self.bias:\n",
        "    # adding bias node to the end of the inpuy_vector\n",
        "    input_vector = np.concatenate( (input_vector,\n",
        "    [self.bias]) )\n",
        "    output_vectors = []\n",
        "    input_vector = np.array(input_vector, ndmin=2).T\n",
        "    target_vector = np.array(target_vector, ndmin=2).T\n",
        "    output_vector1 = np.dot(self.wih,\n",
        "    input_vector)\n",
        "    output_hidden = activation_function(output_vector1)\n",
        "    if self.bias:\n",
        "    output_hidden = np.concatenate((output_hidden,\n",
        "    [[self.bias]]) )\n",
        "    output_vector2 = np.dot(self.who,\n",
        "    output_hidden)\n",
        "    output_network = activation_function(output_vector2)\n",
        "    output_errors = target_vector - output_network\n",
        "    # update the weights:\n",
        "    tmp = output_errors * output_network * (1.0 - output_network)\n",
        "    tmp = self.learning_rate * np.dot(tmp,\n",
        "    output_hidden.T)\n",
        "    self.who += tmp\n",
        "    # calculate hidden errors:\n",
        "    hidden_errors = np.dot(self.who.T,\n",
        "    output_errors)\n",
        "\n",
        "    # update the weights:\n",
        "    tmp = hidden_errors * output_hidden * (1.0 - output_hidden)\n",
        "    if self.bias:\n",
        "      x = np.dot(tmp, input_vector.T)[:-1,:]\n",
        "    else:\n",
        "      x = np.dot(tmp, input_vector.T)\n",
        "    self.wih += self.learning_rate * x\n",
        "  def train(self, data_array,\n",
        "    labels_one_hot_array,\n",
        "    epochs=1,\n",
        "    intermediate_results=False):\n",
        "    intermediate_weights = []\n",
        "    for epoch in range(epochs):\n",
        "      for i in range(len(data_array)):\n",
        "        self.train_single(data_array[i],\n",
        "        labels_one_hot_array[i])\n",
        "        if intermediate_results:\n",
        "          intermediate_weights.append((self.wih.copy(),\n",
        "          self.who.copy()))\n",
        "    return intermediate_weights\n",
        "  def run(self, input_vector):\n",
        "    # input_vector can be tuple, list or ndarray\n",
        "    if self.bias:\n",
        "    # adding bias node to the end of the inpuy_vector\n",
        "      input_vector = np.concatenate( (input_vector,\n",
        "      [self.bias]) )\n",
        "    input_vector = np.array(input_vector, ndmin=2).T\n",
        "    output_vector = np.dot(self.wih,\n",
        "    input_vector)\n",
        "    output_vector = activation_function(output_vector)\n",
        "    if self.bias:\n",
        "      output_vector = np.concatenate( (output_vector,\n",
        "      [[self.bias]]) )\n",
        "\n",
        "    output_vector = np.dot(self.who,\n",
        "    output_vector)\n",
        "    output_vector = activation_function(output_vector)\n",
        "    return output_vector\n",
        "  def evaluate(self, data, labels):\n",
        "    corrects, wrongs = 0, 0\n",
        "    for i in range(len(data)):\n",
        "      res = self.run(data[i])\n",
        "      res_max = res.argmax()\n",
        "      if res_max == labels[i]:\n",
        "        corrects += 1\n",
        "      else:\n",
        "        wrongs += 1\n",
        "    return corrects, wrongs\n",
        "  \n"
      ],
      "metadata": {
        "id": "cBi57wDzofBG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 12\n",
        "network = NeuralNetwork(no_of_in_nodes=image_pixels,\n",
        "no_of_out_nodes=10,\n",
        "no_of_hidden_nodes=100,\n",
        "learning_rate=0.1,\n",
        "bias=None)\n",
        "weights = network.train(train_imgs,\n",
        "train_labels_one_hot,\n",
        "epochs=epochs,\n",
        "intermediate_results=True)\n",
        "for epoch in range(epochs):\n",
        "  print(\"epoch: \", epoch)\n",
        "  network.wih = weights[epoch][0]\n",
        "  network.who = weights[epoch][1]\n",
        "  corrects, wrongs = network.evaluate(train_imgs,\n",
        "  train_labels)\n",
        "  print(\"accuracy train: \", corrects / ( corrects + wrong\n",
        "  s))\n",
        "  corrects, wrongs = network.evaluate(test_imgs,\n",
        "  test_labels)\n",
        "  print(\"accuracy test: \", corrects / ( corrects + wrongs))"
      ],
      "metadata": {
        "id": "tGEHpC_Kpwcu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 12\n",
        "with open(\"nist_tests.csv\", \"w\") as fh_out:\n",
        "  for hidden_nodes in [20, 50, 100, 120, 150]:\n",
        "    for learning_rate in [0.01, 0.05, 0.1, 0.2]:\n",
        "      for bias in [None, 0.5]:\n",
        "      network = NeuralNetwork(no_of_in_nodes=image_pixels,\n",
        "      no_of_out_nodes=10,\n",
        "      no_of_hidden_nodes=hidden_nodes,\n",
        "      learning_rate=learning_rate,\n",
        "      bias=bias)\n",
        "      weights = network.train(train_imgs,\n",
        "      train_labels_one_hot,\n",
        "      epochs=epochs,\n",
        "      intermediate_results=True)\n",
        "        for epoch in range(epochs):\n",
        "        print(\"*\", end=\"\")\n",
        "        network.wih = weights[epoch][0]\n",
        "        network.who = weights[epoch][1]\n",
        "        train_corrects, train_wrongs = network.evaluat\n",
        "        e(train_imgs,\n",
        "        train_labels)\n",
        "        test_corrects, test_wrongs = network.evaluat\n",
        "        e(test_imgs,\n",
        "        test_labels)\n",
        "        outstr = str(hidden_nodes) + \" \" + str(learnin\n",
        "        g_rate) + \" \" + str(bias)\n",
        "        outstr += \" \" + str(epoch) + \" \"\n",
        "        outstr += str(train_corrects / (train_correct\n",
        "        s + train_wrongs)) + \" \"\n",
        "        outstr += str(train_wrongs / (train_corrects\n",
        "        + train_wrongs)) + \" \"\n",
        "        outstr += str(test_corrects / (test_corrects\n",
        "        + test_wrongs)) + \" \"\n",
        "        outstr += str(test_wrongs / (test_corrects + t\n",
        "        est_wrongs))\n",
        "        fh_out.write(outstr + \"\\n\" )\n",
        "        fh_out.flush()\n"
      ],
      "metadata": {
        "id": "MFR2Z38TqUv7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Réseau neuronal avec plusieurs couches cachées"
      ],
      "metadata": {
        "id": "hgDumz67rGQV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.special import expit as activation_function\n",
        "from scipy.stats import truncnorm\n",
        "def truncated_normal(mean=0, sd=1, low=0, upp=10):\n",
        "  return truncnorm((low - mean) / sd,\n",
        "  (upp - mean) / sd,\n",
        "  loc=mean,\n",
        "  scale=sd)\n"
      ],
      "metadata": {
        "id": "i30szCv9rQh5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNetwork:\n",
        "  def __init__(self,\n",
        "    network_structure, # ie. [input_nodes, hidden1_no\n",
        "    des, ... , hidden_n_nodes, output_nodes]\n",
        "    learning_rate,\n",
        "    bias=None\n",
        "    ):\n",
        "    self.structure = network_structure\n",
        "    self.learning_rate = learning_rate\n",
        "    self.bias = bias\n",
        "    self.create_weight_matrices()\n",
        "  def create_weight_matrices(self):\n",
        "    bias_node = 1 if self.bias else 0\n",
        "    self.weights_matrices = []\n",
        "    layer_index = 1\n",
        "    no_of_layers = len(self.structure)\n",
        "    while layer_index < no_of_layers:\n",
        "      nodes_in = self.structure[layer_index-1]\n",
        "      nodes_out = self.structure[layer_index]\n",
        "      n = (nodes_in + bias_node) * nodes_out\n",
        "      rad = 1 / np.sqrt(nodes_in)\n",
        "      X = truncated_normal(mean=2,\n",
        "      sd=1,\n",
        "      low=-rad,\n",
        "      upp=rad)\n",
        "      wm = X.rvs(n).reshape((nodes_out, nodes_in + bias_node))\n",
        "      self.weights_matrices.append(wm)\n",
        "      layer_index += 1\n",
        "  def train(self, input_vector, target_vector):\n",
        "    #\"\"\"\n",
        "    #input_vector and target_vector can be tuple,\n",
        "    #list or ndarray\n",
        "    #\"\"\"\n",
        "    no_of_layers = len(self.structure)\n",
        "    input_vector = np.array(input_vector, ndmin=2).T\n",
        "    layer_index = 0\n",
        "    # The output/input vectors of the various layers:\n",
        "    res_vectors = [input_vector]\n",
        "    while layer_index < no_of_layers - 1:\n",
        "      in_vector = res_vectors[-1]\n",
        "      if self.bias:\n",
        "        # adding bias node to the end of the 'input'_vecto\n",
        "        r\n",
        "        in_vector = np.concatenate( (in_vector,\n",
        "        [[self.bias]]) )\n",
        "        res_vectors[-1] = in_vector\n",
        "      x = np.dot(self.weights_matrices[layer_index],\n",
        "      in_vector)\n",
        "      out_vector = activation_function(x)\n",
        "      # the output of one layer is the input of the next one:\n",
        "      res_vectors.append(out_vector)\n",
        "      layer_index += 1\n",
        "    layer_index = no_of_layers - 1\n",
        "    target_vector = np.array(target_vector, ndmin=2).T\n",
        "    # The input vectors to the various layers\n",
        "\n",
        "    output_errors = target_vector - out_vector\n",
        "    while layer_index > 0:\n",
        "      out_vector = res_vectors[layer_index]\n",
        "      in_vector = res_vectors[layer_index-1]\n",
        "      if self.bias and not layer_index==(no_of_layers-1):\n",
        "        out_vector = out_vector[:-1,:].copy()\n",
        "      tmp = output_errors * out_vector * (1.0 - out_vecto\n",
        "      r)\n",
        "      tmp = np.dot(tmp, in_vector.T)\n",
        "      #if self.bias:\n",
        "      # tmp = tmp[:-1,:]\n",
        "      self.weights_matrices[layer_index-1] += self.learnin\n",
        "      g_rate * tmp\n",
        "      output_errors = np.dot(self.weights_matrices[layer_ind\n",
        "      ex-1].T,\n",
        "      output_errors)\n",
        "      if self.bias:\n",
        "        output_errors = output_errors[:-1,:]\n",
        "      layer_index -= 1\n",
        "  def run(self, input_vector):\n",
        "    # input_vector can be tuple, list or ndarray\n",
        "    no_of_layers = len(self.structure)\n",
        "    if self.bias:\n",
        "      # adding bias node to the end of the inpuy_vector\n",
        "      input_vector = np.concatenate( (input_vector,\n",
        "      [self.bias]) )\n",
        "    in_vector = np.array(input_vector, ndmin=2).T\n",
        "    layer_index = 1\n",
        "    # The input vectors to the various layers\n",
        "    while layer_index < no_of_layers:\n",
        "      x = np.dot(self.weights_matrices[layer_index-1],\n",
        "      in_vector)\n",
        "      out_vector = activation_function(x)\n",
        "      # input vector for next layer\n",
        "      in_vector = out_vector\n",
        "      if self.bias:\n",
        "      in_vector = np.concatenate( (in_vector,\n",
        "      [[self.bias]])\n",
        "      )\n",
        "      layer_index += 1\n",
        "    return out_vector\n",
        "\n",
        "  def evaluate(self, data, labels):\n",
        "    corrects, wrongs = 0, 0\n",
        "    for i in range(len(data)):\n",
        "      res = self.run(data[i])\n",
        "      res_max = res.argmax()\n",
        "      if res_max == labels[i]:\n",
        "        corrects += 1\n",
        "      else:\n",
        "        wrongs += 1\n",
        "    return corrects, wrongs\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "w9wOXaJNrXPH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ANN = NeuralNetwork(network_structure=[image_pixels, 50, 50, 10],\n",
        "learning_rate=0.1,\n",
        "bias=None)\n",
        "for i in range(len(train_imgs)):\n",
        "  ANN.train(train_imgs[i], train_labels_one_hot[i])\n"
      ],
      "metadata": {
        "id": "9x1MpQ2DtgYh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corrects, wrongs = ANN.evaluate(train_imgs, train_labels)\n",
        "print(\"accuracy train: \", corrects / ( corrects + wrongs))\n",
        "corrects, wrongs = ANN.evaluate(test_imgs, test_labels)\n",
        "print(\"accuracy: test\", corrects / ( corrects + wrongs))\n"
      ],
      "metadata": {
        "id": "bX2W5CyTtmVR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}