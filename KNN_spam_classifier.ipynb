{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "KNN-spam-classifier.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOCdPV6OhKFT1Op+ENh03vP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gaussssss/machinelearning-tpe/blob/main/KNN_spam_classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>NoteBook de TPE d'apprentissage artificiel UY I Niveau 3 Info 2021/2022</h1>\n",
        "\n",
        "<h3>Liste des membres du TPE</h3>\n",
        "<ul>\n",
        "  <li>MOMENE TIYA Florian <strong>21S2785</strong></li>\n",
        "  <li>TEIDA NOUTSA Remi Raoul <strong>19M2447</strong></li>\n",
        "  <li>MBOCK NYENGUE Anne Claude <strong>19M2211</strong></li>\n",
        "  <li>EKOH FOUAPON NJIKAM Yvan <strong>19M2198</strong></li>\n",
        "</ul>\n",
        "<br>\n",
        "<strong><i>Par Dr. Melatagia</i></strong>"
      ],
      "metadata": {
        "id": "v5ak1kZkIMkt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import string\n",
        "import nltk\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "nltk.download('stopwords')\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "I2ARefnlLZ47",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1bdf95e4-70d4-463e-8186-a75a5844739b"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data():\n",
        "    print(\"Loading data...\")\n",
        "    data_folder = \"/content/drive/MyDrive/Colab Notebooks/enron2/\"\n",
        "    ham_files_location = os.listdir(f\"{data_folder}ham/\")\n",
        "    spam_files_location = os.listdir(f\"{data_folder}spam/\")\n",
        "    data = []\n",
        "\n",
        "    # Load ham email\n",
        "    for file_path in ham_files_location:\n",
        "        f = open(f\"{data_folder}ham/\" + file_path, \"r\", encoding='iso-8859-1')\n",
        "        text = str(f.read())\n",
        "        data.append([text, \"ham\"])\n",
        "    \n",
        "    # Load spam email\n",
        "    for file_path in spam_files_location:\n",
        "        f = open(f\"{data_folder}spam/\" + file_path, \"r\", encoding='iso-8859-1')\n",
        "        text = str(f.read())\n",
        "        data.append([text, \"spam\"])\n",
        "    \n",
        "    data = np.array(data)\n",
        "    \n",
        "    print(\"flag 1: loaded data\")\n",
        "    return data"
      ],
      "metadata": {
        "id": "NUxW8r22VhsJ"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing data: noise removal\n",
        "def preprocess_data(data):\n",
        "    print(\"Preprocessing data...\")\n",
        "    \n",
        "    punc = string.punctuation           # Punctuation list\n",
        "    sw = stopwords.words('english')     # Stopwords list\n",
        "\n",
        "    for record in data:\n",
        "        # Remove common punctuation and symbols\n",
        "        for item in punc:\n",
        "            record[0] = record[0].replace(item, \"\")\n",
        "            # Lowercase all letters and remove stopwords \n",
        "          \n",
        "        splittedWords = record[0].split()\n",
        "        newText = \"\"\n",
        "        for word in splittedWords:\n",
        "            if word not in sw:\n",
        "                word = word.lower()\n",
        "                newText = newText + \" \" + word  \n",
        "        record[0] = newText\n",
        "        \n",
        "    print(\"flag 2: preprocessed data\")        \n",
        "    return data"
      ],
      "metadata": {
        "id": "h2Zt1841flJn"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting original dataset into training dataset and test dataset\n",
        "def split_data(data):\n",
        "    print(\"Splitting data...\")\n",
        "    \n",
        "    features = data[:, 0]   # array containing all email text bodies\n",
        "    labels = data[:, 1]     # array containing corresponding labels\n",
        "    print(labels)\n",
        "    training_data, test_data, training_labels, test_labels =\\\n",
        "        train_test_split(features, labels, test_size = 0.27, random_state = 42)\n",
        "    \n",
        "    print(\"flag 3: splitted data\")\n",
        "    return training_data, test_data, training_labels, test_labels"
      ],
      "metadata": {
        "id": "g3Cs4ps-gBEh"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_count(text):\n",
        "    wordCounts = dict()\n",
        "    for word in text.split():\n",
        "        if word in wordCounts:\n",
        "            wordCounts[word] += 1\n",
        "        else:\n",
        "            wordCounts[word] = 1\n",
        "    \n",
        "    return wordCounts\n",
        "\n",
        "def euclidean_difference(test_WordCounts, training_WordCounts):\n",
        "    total = 0\n",
        "    for word in test_WordCounts:\n",
        "        if word in test_WordCounts and word in training_WordCounts:\n",
        "            total += (test_WordCounts[word] - training_WordCounts[word])**2\n",
        "            del training_WordCounts[word]\n",
        "        else:\n",
        "            total += test_WordCounts[word]**2\n",
        "    for word in training_WordCounts:\n",
        "        total += training_WordCounts[word]**2\n",
        "    return total**0.5"
      ],
      "metadata": {
        "id": "NOs6wERhgH1X"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_class(selected_Kvalues):\n",
        "    spam_count = 0\n",
        "    ham_count = 0\n",
        "\n",
        "    for value in selected_Kvalues:\n",
        "        if value[0] == \"spam\":\n",
        "            spam_count += 1\n",
        "        else:\n",
        "            ham_count += 1\n",
        "    if spam_count > ham_count:\n",
        "        return \"spam\"\n",
        "    else:\n",
        "        return \"ham\""
      ],
      "metadata": {
        "id": "dLNYbwE6hRMC"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def knn_classifier(training_data, training_labels, test_data, K, tsize):\n",
        "    print(\"Running KNN Classifier...\")\n",
        "    \n",
        "    result = []\n",
        "    counter = 1\n",
        "\n",
        "    # word counts for training email\n",
        "    training_WordCounts = [] \n",
        "    for training_text in training_data:\n",
        "        training_WordCounts.append(get_count(training_text))\n",
        "    for test_text in test_data:\n",
        "        similarity = [] # List of euclidean distances\n",
        "        test_WordCounts = get_count(test_text)  # word counts for test email\n",
        "        # Getting euclidean difference \n",
        "        for index in range(len(training_data)):\n",
        "            euclidean_diff =\\\n",
        "                euclidean_difference(test_WordCounts, training_WordCounts[index])\n",
        "            similarity.append([training_labels[index], euclidean_diff])\n",
        "\n",
        "        # Sort list in ascending order based on euclidean difference\n",
        "        similarity = sorted(similarity, key = lambda i:i[1])\n",
        "\n",
        "        # Select K nearest neighbours\n",
        "        selected_Kvalues = [] \n",
        "        for i in range(K):\n",
        "            selected_Kvalues.append(similarity[i])\n",
        "        # Predicting the class of email\n",
        "        result.append(get_class(selected_Kvalues))\n",
        "    return result"
      ],
      "metadata": {
        "id": "5GWwqO9phk7M"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main(K):\n",
        "    data = load_data()\n",
        "    data = preprocess_data(data)\n",
        "    training_data, test_data, training_labels, test_labels = split_data(data)\n",
        "    tsize = len(test_data)\n",
        "    result = knn_classifier(training_data, training_labels, test_data[:tsize], K, tsize) \n",
        "    accuracy = accuracy_score(test_labels[:tsize], result)\n",
        "    print(\"training data size\\t: \" + str(len(training_data)))\n",
        "    print(\"test data size\\t\\t: \" + str(len(test_data)))\n",
        "    print(\"K value\\t\\t\\t\\t: \" + str(K))\n",
        "    print(\"Samples tested\\t\\t: \" + str(tsize))\n",
        "    print(\"% accuracy\\t\\t\\t: \" + str(accuracy * 100))\n",
        "    print(\"Number correct\\t\\t: \" + str(int(accuracy * tsize)))\n",
        "    print(\"Number wrong\\t\\t: \" + str(int((1 - accuracy) * tsize)))"
      ],
      "metadata": {
        "id": "YB-q89NOiN5J"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main(11)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h3-nSbNFieKG",
        "outputId": "9da0145a-b354-4e2b-e66e-8b369bec46ab"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data...\n",
            "flag 1: loaded data\n",
            "Preprocessing data...\n",
            "flag 2: preprocessed data\n",
            "Splitting data...\n",
            "['ham' 'ham' 'ham' ... 'spam' 'spam' 'spam']\n",
            "flag 3: splitted data\n",
            "Running KNN Classifier...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "9VUmjuzhiiQK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}